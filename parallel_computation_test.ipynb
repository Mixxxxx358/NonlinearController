{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepSI\n",
    "import numpy as np\n",
    "from casadi import *\n",
    "from NonlinearController.model_utils import *\n",
    "import time\n",
    "import torch\n",
    "from functorch import jacrev, jacfwd, vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_trap(x0,u0,nx,nu,ny,Jfx,Jfu,Jhx,stages):\n",
    "    # FUNCTION LAMBDA_SIMPSON\n",
    "    # Simpson rule integrator between 0 and 1 with chosen resolution (stages)\n",
    "    # used to get A,B matrices symbolically to be used at gridpoints\n",
    "    \n",
    "    A = np.zeros([nx,nx])\n",
    "    B = np.zeros([nx,nu])\n",
    "    C = np.zeros([ny,nx])\n",
    "    lambda0 = 0\n",
    "    dlam = 1/stages\n",
    "\n",
    "    x = lambda lam: lam*x0\n",
    "    u = lambda lam: lam*u0\n",
    "\n",
    "    for i in np.arange(stages):\n",
    "        A = A + dlam*1/2*(Jfx(x(lambda0), u(lambda0)) + Jfx(x(lambda0+dlam), u(lambda0+dlam)))\n",
    "        B = B + dlam*1/2*(Jfu(x(lambda0), u(lambda0)) + Jfu(x(lambda0+dlam), u(lambda0+dlam)))\n",
    "        C = C + dlam*1/2*(Jhx(x(lambda0), u(lambda0)) + Jhx(x(lambda0+dlam), u(lambda0+dlam)))\n",
    "        lambda0 = lambda0 + dlam\n",
    "            \n",
    "    return A,B,C\n",
    "\n",
    "def lambda_simpson(x,u,nx,nu,ny,Jfx,Jfu,Jhx,stages):\n",
    "    # FUNCTION LAMBDA_SIMPSON\n",
    "    # Simpson rule integrator between 0 and 1 with chosen resolution (stages)\n",
    "    # used to get A,B matrices symbolically to be used at gridpoints\n",
    "    \n",
    "    A = np.zeros([nx,nx])\n",
    "    B = np.zeros([nx,nu])\n",
    "    C = np.zeros([ny,nx])\n",
    "    lambda0 = 0\n",
    "    dlam = 1/stages\n",
    "\n",
    "    for i in np.arange(stages):\n",
    "        A = A + dlam*1/6*(Jfx(lambda0*x,lambda0*u) + 4*Jfx((lambda0+dlam/2)*x,(lambda0+dlam/2)*u) + Jfx((lambda0+dlam)*x,(lambda0+dlam)*u))\n",
    "        B = B + dlam*1/6*(Jfu(lambda0*x,lambda0*u) + 4*Jfu((lambda0+dlam/2)*x,(lambda0+dlam/2)*u) + Jfu((lambda0+dlam)*x,(lambda0+dlam)*u))\n",
    "        C = C + dlam*1/6*(Jhx(lambda0*x,lambda0*u) + 4*Jhx((lambda0+dlam/2)*x,(lambda0+dlam/2)*u) + Jhx((lambda0+dlam)*x,(lambda0+dlam)*u))\n",
    "        lambda0 = lambda0 + dlam\n",
    "            \n",
    "    return A,B,C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = deepSI.load_system(\"NonlinearController/trained_models/unbalanced/ObserverUnbalancedDisk_dt01_nab_4_SNR_30_e250\")\n",
    "n_stages = 20\n",
    "Nc = 10\n",
    "n_int_comp = 3\n",
    "threads = 8\n",
    "\n",
    "# declared sym variables\n",
    "nx = model.nx\n",
    "x = MX.sym(\"x\",nx,1)\n",
    "nu = model.nu if model.nu is not None else 1\n",
    "u = MX.sym(\"u\",nu,1)\n",
    "ny = model.ny if model.ny is not None else 1\n",
    "\n",
    "# convert torch nn to casadi function\n",
    "x_rhs = CasADi_Fn(model, x, u)\n",
    "f = Function('f', [x, u], [x_rhs])\n",
    "y_rhs = CasADi_Hn(model, x)\n",
    "h = Function('h', [x], [y_rhs])\n",
    "\n",
    "correction_f = f(np.zeros((nx,1)), 0)\n",
    "x_rhs_c = x_rhs - correction_f\n",
    "correction_h = h(np.zeros((nx,1)))\n",
    "y_rhs_c = y_rhs - correction_h\n",
    "\n",
    "Jfx = Function(\"Jfx\", [x, u], [jacobian(x_rhs_c,x)])\n",
    "Jfu = Function(\"Jfu\", [x, u], [jacobian(x_rhs_c,u)])\n",
    "Jhx = Function(\"Jhx\", [x, u], [jacobian(y_rhs_c,x)])\n",
    "\n",
    "x0 = MX.sym(\"x0\",nx,1)\n",
    "u0 = MX.sym(\"u0\",nu,1)\n",
    "\n",
    "[A_sym, B_sym, C_sym] = lambda_simpson(x0,u0,nx,nu,ny,Jfx,Jfu,Jhx,n_stages)\n",
    "lpv_A = Function(\"get_A\",[x0,u0],[A_sym])\n",
    "lpv_B = Function(\"get_B\",[x0,u0],[B_sym])\n",
    "lpv_C = Function(\"get_C\",[x0,u0],[C_sym])\n",
    "\n",
    "lpv_A_Nc = lpv_A.map(Nc, \"thread\", threads)\n",
    "lpv_B_Nc = lpv_B.map(Nc, \"thread\", threads)\n",
    "lpv_C_Nc = lpv_C.map(Nc, \"thread\", threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = np.array([])\n",
    "lambda0 = 0\n",
    "dlam = 1/n_stages\n",
    "\n",
    "for i in np.arange(n_stages):\n",
    "    Lambda = np.hstack((Lambda, lambda0, lambda0 + dlam/2, lambda0 + dlam)) # Simpson\n",
    "    # Lambda = np.hstack((Lambda, lambda0, lambda0 + dlam)) # Trapezium\n",
    "    lambda0 = lambda0 + dlam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpv_A_lam = Jfx.map(Nc*n_stages*n_int_comp, \"thread\", threads)\n",
    "lpv_B_lam = Jfu.map(Nc*n_stages*n_int_comp, \"thread\", threads)\n",
    "lpv_C_lam = Jhx.map(Nc*n_stages*n_int_comp, \"thread\", threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    dev = \"cuda:0\" \n",
    "else: \n",
    "    dev = \"cpu\" \n",
    "device = torch.device(dev) \n",
    "JacF = torch.vmap(torch.func.jacrev(model.fn.to(device).float(), argnums=(0,1)))\n",
    "JacH = torch.vmap(torch.func.jacrev(model.hn.to(device).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functorch: 5.178942680358887\n",
      "CasADi Nc: 0.0\n",
      "CasADi stages: 14.02291464805603\n"
     ]
    }
   ],
   "source": [
    "n_iter = 1000\n",
    "torch.set_num_threads(threads)\n",
    "\n",
    "func_end_time = 0.0\n",
    "cas_end_time = 0.0\n",
    "stages_end_time = 0.0\n",
    "\n",
    "for i in range(n_iter):\n",
    "    X = np.random.rand(nx,Nc)\n",
    "    U = np.random.rand(nu,Nc)\n",
    "\n",
    "    Xlam = np.kron(X, Lambda)\n",
    "    Ulam = np.kron(U, Lambda)\n",
    "\n",
    "    batch_size = Nc*n_stages*n_int_comp\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    x = torch.reshape(torch.tensor(Xlam[np.newaxis].T, device=device),(batch_size,1,2)).float()\n",
    "    u = torch.reshape(torch.tensor(Ulam[np.newaxis].T, device=device),(batch_size,1,1)).float()\n",
    "\n",
    "    fA, fB = JacF(x,u)\n",
    "    fC = JacH(x)\n",
    "\n",
    "    temp1 = fA.cpu()\n",
    "    temp2 = fB.cpu()\n",
    "    temp3 = fC.cpu()\n",
    "\n",
    "    func_end_time += time.time() - start_time\n",
    "\n",
    "    # start_time = time.time()\n",
    "\n",
    "    # pA = lpv_A_Nc(X,U)\n",
    "    # pB = lpv_B_Nc(X,U)\n",
    "    # pC = lpv_C_Nc(X,U)\n",
    "\n",
    "    # cas_end_time += time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    tA = lpv_A_lam(Xlam,Ulam)\n",
    "    tB = lpv_B_lam(Xlam,Ulam)\n",
    "    tC = lpv_C_lam(Xlam,Ulam)\n",
    "\n",
    "    stages_end_time += time.time() - start_time\n",
    "\n",
    "print(\"functorch: \" + str(func_end_time))\n",
    "print(\"CasADi Nc: \" + str(cas_end_time))\n",
    "print(\"CasADi stages: \" + str(stages_end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pA = lpv_A_Nc(X,U)\n",
    "pB = lpv_B_Nc(X,U)\n",
    "pC = lpv_C_Nc(X,U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Loops: 1.053025245666504\n",
      "Torch short: 0.03500032424926758\n"
     ]
    }
   ],
   "source": [
    "n_iter = 1\n",
    "\n",
    "list_A = np.zeros([Nc*nx, nx])\n",
    "list_B = np.zeros([Nc*nx, nu])\n",
    "list_C = np.zeros([Nc*ny, nx])\n",
    "\n",
    "mult_A = np.stack((np.ones((nx,nx)), np.ones((nx,nx))*4, np.ones((nx,nx))))\n",
    "mult_B = np.stack((np.ones((nx,nu)), np.ones((nx,nu))*4, np.ones((nx,nu))))\n",
    "mult_C = np.vstack((np.ones((ny,nx)), np.ones((ny,nx))*4, np.ones((ny,nx))))\n",
    "\n",
    "start_time = time.time()\n",
    "for t in range(n_iter):\n",
    "\n",
    "    for j in range(Nc):\n",
    "        A = np.zeros((nx,nx))\n",
    "        B = np.zeros([nx,nu])\n",
    "        C = np.zeros((ny,nx))\n",
    "\n",
    "        lambda0 = 0\n",
    "\n",
    "        for i in range(n_stages):\n",
    "            an = fA[n_int_comp*n_stages*j+(i)*n_int_comp:n_int_comp*n_stages*j+(i+1)*n_int_comp,0,:,0,:].detach().cpu().numpy()\n",
    "            A = A + dlam*1/6*np.sum(np.multiply(mult_A, an), axis=0)\n",
    "\n",
    "            bn = fB[n_int_comp*n_stages*j+(i)*n_int_comp:n_int_comp*n_stages*j+(i+1)*n_int_comp,0,:,0,:].detach().cpu().numpy()\n",
    "            B = B + dlam*1/6*np.sum(np.multiply(mult_B, bn), axis=0)\n",
    "\n",
    "            cn = fC[n_int_comp*n_stages*j+(i)*n_int_comp:n_int_comp*n_stages*j+(i+1)*n_int_comp,0,0,:].detach().cpu().numpy()\n",
    "            C = C + dlam*1/6*np.sum(np.multiply(mult_C, cn), axis=0)[np.newaxis]\n",
    "\n",
    "            lambda0 = lambda0 + dlam\n",
    "\n",
    "        list_A[nx*(j):nx*(j+1),:] = A.copy()\n",
    "        list_B[nx*(j):nx*(j+1),:] = B.copy()\n",
    "        list_C[ny*(j):ny*(j+1),:] = C.copy()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"For Loops: \" + str(end_time - start_time))\n",
    "\n",
    "\n",
    "length_jac = n_stages*n_int_comp*Nc\n",
    "\n",
    "mult_fA = torch.from_numpy(np.tile(np.vstack((np.ones((1,nx,nx)), 4*np.ones((1,nx,nx)), np.ones((1,nx,nx)))),(n_stages*Nc,1,1))).cuda()*dlam/6\n",
    "mult_fB = torch.from_numpy(np.tile(np.vstack((np.ones((1,nx,nu)), 4*np.ones((1,nx,nu)), np.ones((1,nx,nu)))),(n_stages*Nc,1,1))).cuda()*dlam/6\n",
    "mult_fC = torch.from_numpy(np.tile(np.vstack((np.ones((1,ny,nx)), 4*np.ones((1,ny,nx)), np.ones((1,ny,nx)))),(n_stages*Nc,1,1))).cuda()*dlam/6\n",
    "\n",
    "list_fA = np.zeros([Nc*nx, nx])\n",
    "list_fB = np.zeros([Nc*nx, nu])\n",
    "list_fC = np.zeros([Nc*ny, nx])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for t in range(n_iter):\n",
    "    tempA = torch.tensor_split(torch.mul(fA.view((length_jac,nx,nx)), mult_fA).detach(), Nc)\n",
    "    tempB = torch.tensor_split(torch.mul(fB.view((length_jac,nx,nu)), mult_fB).detach(), Nc)\n",
    "    tempC = torch.tensor_split(torch.mul(fC.view((length_jac,ny,nx)), mult_fC).detach(), Nc)\n",
    "    for i in range(Nc):\n",
    "        list_fA[nx*(i):nx*(i+1),:] = torch.sum(tempA[i], axis=0).cpu().numpy()\n",
    "        list_fB[nx*(i):nx*(i+1),:] = torch.sum(tempB[i], axis=0).cpu().numpy()\n",
    "        list_fC[ny*(i):ny*(i+1),:] = torch.sum(tempC[i], axis=0).cpu().numpy()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Torch short: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(list_A, list_fA), np.allclose(list_B, list_fB), np.allclose(list_C, list_fC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_A_1 = np.zeros([Nc*nx, nx])\n",
    "list_B_1 = np.zeros([Nc*nx, nx])\n",
    "list_C_1 = np.zeros([Nc*ny, nx])\n",
    "for i in range(Nc):\n",
    "    list_A_1[(nx*i):(nx*i+nx),:] = pA[:,i*nx:(i+1)*nx]\n",
    "for i in range(Nc):\n",
    "    list_B_1[(nx*i):(nx*i+nx),:] = pB[:,i*nu:(i+1)*nu]\n",
    "for i in range(Nc):\n",
    "    list_C_1[(ny*i):(ny*i+ny),:] = pC[:,i*nx:(i+1)*nx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(list_A_1, list_A), np.allclose(list_B_1, list_B), np.allclose(list_C_1, list_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
